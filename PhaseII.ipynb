{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ4Xt1uf1LT7"
   },
   "source": [
    "# Final Project Phase 2 Summary - Emily McNichols and Charlotte Hettrich\n",
    "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 2 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
    "\n",
    "Note: To edit a Markdown cell, double-click on its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjB_SbWY1LUB"
   },
   "source": [
    "## Jupyter Notebook Quick Tips\n",
    "Here are some quick formatting tips to get you started with Jupyter Notebooks. This is by no means exhaustive, and there are plenty of articles to highlight other things that can be done. We recommend using HTML syntax for Markdown but there is also Markdown syntax that is more streamlined and might be preferable. \n",
    "<a href = \"https://towardsdatascience.com/markdown-cells-jupyter-notebook-d3bea8416671\">Here's an article</a> that goes into more detail. (Double-click on cell to see syntax)\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2\n",
    "### Heading 3\n",
    "#### Heading 4\n",
    "<br>\n",
    "<b>BoldText</b> or <i>ItalicText</i>\n",
    "<br> <br>\n",
    "Math Formulas: $x^2 + y^2 = 1$\n",
    "<br> <br>\n",
    "Line Breaks are done using br enclosed in < >.\n",
    "<br><br>\n",
    "Hyperlinks are done with: <a> https://www.google.com </a> or \n",
    "<a href=\"http://www.google.com\">Google</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb9oVjpRDswQ"
   },
   "source": [
    "# Data Collection and Cleaning\n",
    "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file. \n",
    "\n",
    "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp7Pm-Suh3d"
   },
   "source": [
    "## Data Sources\n",
    "Include sources (as links) to your datasets. Add any additional data sources if needed. Clearly indicate if a data source is different from one submitted in your Phase I, as we will check that it satisfies the requirements.\n",
    "*   Downloaded Dataset Source: https://nam12.safelinks.protection.outlook.com/GetUrlReputation\n",
    "*   Web Collection #1 Source: https://oasis.state.ga.us/oasis/webquery/qryPregnancy.aspx\n",
    "*   Web Collection #2 Source: https://crime-data-explorer.fr.cloud.gov/pages/docApi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mRjxZDbE1tj"
   },
   "source": [
    "## Downloaded Dataset Requirement\n",
    "\n",
    "Fill in the predefined functions with your data scraping/parsing code. You may modify/rename each function as you seem fit, but you must provide at least 3 separate functions that clean each of your required datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0p5xxmqzFGrO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               LONG_SCHOOL_YEAR  STUDENT_COUNT_ALL  \\\n",
      "SCHOOL_DSTRCT_NM                                                     \n",
      "Appling County                          2010-11               3848   \n",
      "Atkinson County                         2010-11               1892   \n",
      "Bacon County                            2010-11               2137   \n",
      "Baker County                            2010-11                379   \n",
      "Baldwin County                          2010-11               5951   \n",
      "...                                         ...                ...   \n",
      "Trion City                              2019-20               1330   \n",
      "Valdosta City                           2019-20               9307   \n",
      "Vidalia City                            2019-20               2579   \n",
      "State Schools                           2019-20                350   \n",
      "Department of Juvenile Justice          2019-20                495   \n",
      "\n",
      "                                STUDENT_COUNT_INDIAN  STUDENT_COUNT_ASIAN  \\\n",
      "SCHOOL_DSTRCT_NM                                                            \n",
      "Appling County                                     0                   34   \n",
      "Atkinson County                                    0                    0   \n",
      "Bacon County                                       0                    0   \n",
      "Baker County                                       0                    0   \n",
      "Baldwin County                                     0                   74   \n",
      "...                                              ...                  ...   \n",
      "Trion City                                         0                    0   \n",
      "Valdosta City                                     19                  109   \n",
      "Vidalia City                                       0                   22   \n",
      "State Schools                                      0                   10   \n",
      "Department of Juvenile Justice                     0                    0   \n",
      "\n",
      "                                STUDENT_COUNT_BLACK  STUDENT_COUNT_WHITE  \\\n",
      "SCHOOL_DSTRCT_NM                                                           \n",
      "Appling County                                  904                 2360   \n",
      "Atkinson County                                 334                  894   \n",
      "Bacon County                                    417                 1437   \n",
      "Baker County                                    280                   61   \n",
      "Baldwin County                                 3787                 1820   \n",
      "...                                             ...                  ...   \n",
      "Trion City                                       17                  986   \n",
      "Valdosta City                                  7024                 1201   \n",
      "Vidalia City                                   1279                  974   \n",
      "State Schools                                   169                   89   \n",
      "Department of Juvenile Justice                  383                   73   \n",
      "\n",
      "                                STUDENT_COUNT_HISPANI  STUDENT_COUNT_MULTI  \\\n",
      "SCHOOL_DSTRCT_NM                                                             \n",
      "Appling County                                    452                   92   \n",
      "Atkinson County                                   629                   30   \n",
      "Bacon County                                      212                   63   \n",
      "Baker County                                       29                    0   \n",
      "Baldwin County                                    117                  147   \n",
      "...                                               ...                  ...   \n",
      "Trion City                                        277                   41   \n",
      "Valdosta City                                     674                  280   \n",
      "Vidalia City                                      191                  109   \n",
      "State Schools                                      77                    0   \n",
      "Department of Juvenile Justice                     28                    0   \n",
      "\n",
      "                                STUDENT_COUNT_FEMALE  STUDENT_COUNT_MALE  \n",
      "SCHOOL_DSTRCT_NM                                                          \n",
      "Appling County                                  1789                2059  \n",
      "Atkinson County                                  945                 947  \n",
      "Bacon County                                    1058                1079  \n",
      "Baker County                                     213                 166  \n",
      "Baldwin County                                  2929                3022  \n",
      "...                                              ...                 ...  \n",
      "Trion City                                       629                 701  \n",
      "Valdosta City                                   4628                4679  \n",
      "Vidalia City                                    1286                1293  \n",
      "State Schools                                    133                 217  \n",
      "Department of Juvenile Justice                    47                 448  \n",
      "\n",
      "[2028 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'education_data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ebe294d29fbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;31m############ Function Call ############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m \u001b[0mdata_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-ebe294d29fbc>\u001b[0m in \u001b[0;36mdata_parser\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;31m# total rows after cleaning = 8,353\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'education_data.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0mtotals_attend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'total_attendance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mattend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'attendance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, **engine_kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Append mode is not supported with xlsxwriter!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, **engine_kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIOHandles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"copression\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m    811\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'education_data.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def data_parser():\n",
    "\n",
    "##### ATTENDANCE DATA #####\n",
    "\n",
    "    attend_2020 = pd.read_csv(\"attendance2020.csv\")\n",
    "    attend_2019 = pd.read_csv(\"attendance2019.csv\")\n",
    "    attend_2018 = pd.read_csv(\"attendance2018.csv\")\n",
    "    attend_2017 = pd.read_csv(\"attendance2017.csv\")\n",
    "    attend_2016 = pd.read_csv(\"attendance2016.csv\")\n",
    "    attend_2015 = pd.read_csv(\"attendance2015.csv\")\n",
    "    attend_2014 = pd.read_csv(\"attendance2014.csv\")\n",
    "    attend_2013 = pd.read_csv(\"attendance2013.csv\")\n",
    "    attend_2012 = pd.read_csv(\"attendance2012.csv\") # had to convert from xlsx to csv\n",
    "    attend_2011 = pd.read_csv(\"attendance2011.csv\")\n",
    "\n",
    "    # print(attend_2020.head())\n",
    "\n",
    "    attend_dfs = [attend_2011, attend_2012, attend_2013, attend_2014, attend_2015, attend_2016, attend_2017, \n",
    "                  attend_2018, attend_2019, attend_2020]\n",
    "\n",
    "    attend_merged = pd.concat(attend_dfs)\n",
    "    attend_columns = list(attend_merged.columns)\n",
    "    # print(attend_columns)\n",
    "    # print(attend_merged)\n",
    "\n",
    "    attend_filtered_rows = attend_merged[attend_merged[\"INSTN_NUMBER\"] == 'ALL']\n",
    "    # print(attend_filtered_rows)\n",
    "\n",
    "    attend_filtered_cols = attend_filtered_rows[[\"LONG_SCHOOL_YEAR\", \"SCHOOL_DSTRCT_NM\",\n",
    "        \"STUDENT_COUNT_ALL\", \"STUDENT_COUNT_INDIAN\", \"STUDENT_COUNT_ASIAN\", \"STUDENT_COUNT_BLACK\",\n",
    "        \"STUDENT_COUNT_WHITE\", \"STUDENT_COUNT_HISPANI\", \"STUDENT_COUNT_MULTI\", \"STUDENT_COUNT_FEMALE\",\n",
    "        \"STUDENT_COUNT_MALE\"]]\n",
    "    attend_filtered_cols.set_index(\"SCHOOL_DSTRCT_NM\", inplace = True)\n",
    "    # print(attend_filtered_cols)\n",
    "\n",
    "    totals_attend = attend_filtered_cols.loc[\"All Column Values\"]\n",
    "    # print(totals_attend) # total number of attendance by year\n",
    "\n",
    "    attend = attend_filtered_cols.drop(\"All Column Values\")\n",
    "    print(attend) # attendance based on county/city schools located in GA throughout the years\n",
    "\n",
    "##### DROPOUT DATA #####\n",
    "\n",
    "    dropout_2020 = pd.read_csv(\"dropout_2020.csv\")\n",
    "    dropout_2019 = pd.read_csv(\"dropout_2019.csv\")\n",
    "    dropout_2018 = pd.read_csv(\"dropout_2018.csv\")\n",
    "    dropout_2017 = pd.read_csv(\"dropout_2017.csv\")\n",
    "    dropout_2016 = pd.read_csv(\"dropout_2016.csv\")\n",
    "    dropout_2015 = pd.read_csv(\"dropout_2015.csv\")\n",
    "    dropout_2014 = pd.read_csv(\"dropout_2014.csv\")\n",
    "    dropout_2013 = pd.read_csv(\"dropout_2013.csv\")\n",
    "    dropout_2012 = pd.read_csv(\"dropout_2012.csv\")\n",
    "    dropout_2011 = pd.read_csv(\"dropout_2011.csv\")\n",
    "\n",
    "    # print(dropout_2020.head())\n",
    "\n",
    "    dropout_dfs = [dropout_2011, dropout_2012, dropout_2013, dropout_2014, \n",
    "                  dropout_2015, dropout_2016, dropout_2017, dropout_2018, \n",
    "                  dropout_2019, dropout_2020]\n",
    "\n",
    "    dropout_merged = pd.concat(dropout_dfs)\n",
    "    # print(dropout_merged)\n",
    "\n",
    "    dropout_columns = list(dropout_merged.columns)\n",
    "    # print(dropout_columns)\n",
    "\n",
    "    dropout_filtered = dropout_merged[(dropout_merged[\"INSTN_NUMBER\"] == \"ALL\") & (dropout_merged[\"LABEL_LVL_1_DESC\"] == \"7-12 Drop Outs -ALL Students\")]\n",
    "    # print(dropout_filtered)\n",
    "\n",
    "    dropout_filtered = dropout_filtered.drop(['DETAIL_LVL_DESC', 'SCHOOL_DSTRCT_CD', 'INSTN_NUMBER', 'GRADES_SERVED_DESC', 'LABEL_LVL_1_DESC'], axis = 1)\n",
    "    # print(dropout_filtered)\n",
    "\n",
    "    dropout_filtered = dropout_filtered.set_index(\"SCHOOL_DSTRCT_NM\")\n",
    "    dropout_filtered = dropout_filtered.dropna()\n",
    "\n",
    "    dropout = dropout_filtered \n",
    "    # print(dropout)  # dropouts and dropout rate based on each county/city throughout the years\n",
    "\n",
    "    total_dropout = dropout_filtered.groupby(\"LONG_SCHOOL_YEAR\").agg({\"PROGRAM_TOTAL\": \"mean\", \"PROGRAM_PERCENT\": \"mean\"})\n",
    "    # print(total_dropout)  # average dropouts and dropout rate each year\n",
    "\n",
    "\n",
    "##### HOPE DATA #####\n",
    "\n",
    "    hope_2011 = pd.read_csv(\"Hope_Eligible_2011_FEB_24_2020.csv\")\n",
    "    hope_2012 = pd.read_csv(\"Hope_Eligible_2012_FEB_24_2020.csv\")\n",
    "    hope_2013 = pd.read_csv(\"Hope_Eligible_2013_FEB_24_2020.csv\")\n",
    "    hope_2014 = pd.read_csv(\"Hope_Eligible_2014_FEB_24_2020.csv\")\n",
    "    hope_2015 = pd.read_csv(\"Hope_Eligible_2015_FEB_24_2020.csv\")\n",
    "    hope_2016 = pd.read_csv(\"Hope_Eligible_2016_FEB_24_2020.csv\")\n",
    "    hope_2017 = pd.read_csv(\"Hope_Eligible_2017_FEB_24_2020.csv\")\n",
    "    hope_2018 = pd.read_csv(\"Hope_Eligible_2018_FEB_24_2020.csv\")\n",
    "    hope_2019 = pd.read_csv(\"Hope_Eligible_2019_FEB_24_2020.csv\")\n",
    "    hope_2020 = pd.read_csv(\"HOPE_ELIGIBILITY_2020_JUN_21_2021.csv\")\n",
    "\n",
    "    hope_dfs = [hope_2011, hope_2012, hope_2013, hope_2014, hope_2015, \n",
    "                hope_2016, hope_2017, hope_2018, hope_2019, hope_2020]\n",
    "    hope_merged = pd.concat(hope_dfs)\n",
    "    # print(hope_merged)\n",
    "\n",
    "    hope_columns = list(hope_merged.columns)\n",
    "    # print(hope_columns)\n",
    "\n",
    "    hope_filtered = hope_merged[(hope_merged[\"INSTN_NUMBER\"] == \"ALL\") & (hope_merged[\"HOPE_ELIGIBLE\"] != \"TFS\")]\n",
    "    # print(hope_filtered)\n",
    "\n",
    "    hope_filtered = hope_filtered.drop([\"SCHOOL_DISTRCT_CD\", \"INSTN_NUMBER\", \"INSTN_NAME\"], axis = 1)\n",
    "    # print(hope_filtered)\n",
    "\n",
    "    hope_filtered = hope_filtered.dropna()\n",
    "\n",
    "    hope = hope_filtered.set_index(\"SCHOOL_DSTRCT_NM\")\n",
    "\n",
    "    total_hope = hope.loc[\"All Systems\"]\n",
    "    # print(total_hope)  # total hope eligible students by year\n",
    "\n",
    "    hope = hope.drop(\"All Systems\")\n",
    "    # print(hope)  # hope eligible students by county/city per year\n",
    "\n",
    "\n",
    "##### HIGH SCHOOL COMPLETER DATA #####\n",
    "\n",
    "    completer_2011 = pd.read_csv(\"HS_Completers_2011_MAR_23_2020.csv\")\n",
    "    completer_2012 = pd.read_csv(\"HS_Completers_2012_Jan_15th_2015.csv\")\n",
    "    completer_2013 = pd.read_csv(\"HS_Completers_2013_Jan_15th_2015.csv\")\n",
    "    completer_2014 = pd.read_csv(\"HS_Completers_2014_Jan_15th_2015.csv\")\n",
    "    completer_2015 = pd.read_csv(\"HS_Completers_2015_DEC_1st_2016.csv\")\n",
    "    completer_2016 = pd.read_csv(\"HS_Completers_2016_Feb_2_2017.csv\")\n",
    "    completer_2017 = pd.read_csv(\"HS_Completers_2017_DEC_1st_2017.csv\")\n",
    "    completer_2018 = pd.read_csv(\"HS_Completers_2018_DEC_10th_2018.csv\")\n",
    "    completer_2019 = pd.read_csv(\"HS_Completers_2019_Dec2nd_2019.csv\")\n",
    "    completer_2020 = pd.read_csv(\"HS_Completers_2020_Dec112020.csv\")\n",
    "\n",
    "    completer_dfs = [completer_2011, completer_2012, completer_2013, \n",
    "                    completer_2014, completer_2015, completer_2016, \n",
    "                    completer_2017, completer_2018, completer_2019, \n",
    "                    completer_2020]\n",
    "    completer_merged = pd.concat(completer_dfs)\n",
    "    # print(completer_merged)\n",
    "\n",
    "    completer_columns = list(completer_merged.columns)\n",
    "    # print(completer_columns)\n",
    "\n",
    "    completer_filtered = completer_merged[(completer_merged[\"INSTN_NAME\"] == \"All Column Values\")\n",
    "        & (completer_merged[\"LABEL_LVL_5_CD\"] == \"Total\") & (completer_merged[\"PROGRAM_TOTAL\"] != \"TFS\")]\n",
    "    # print(completer_filtered)\n",
    "\n",
    "    completer_filtered = completer_filtered.drop([\"SCHOOL_DSTRCT_CD\", \"INSTN_NUMBER\",\n",
    "            \"INSTN_NAME\", \"LABEL_SORT_ORDER\", \"COMPLETER_TYPE\"], axis = 1)\n",
    "    # print(completer_filtered)\n",
    "\n",
    "    completer_filtered = completer_filtered.dropna()\n",
    "    # print(completer_filtered)\n",
    "\n",
    "    completer = completer_filtered.set_index(\"SCHOOL_DSTRCT_NM\")\n",
    "\n",
    "    total_completer = completer.loc[\"All Column Values\"]\n",
    "    total_completer = total_completer.drop(\"PROGRAM_PERCENT\", axis = 1)\n",
    "    total_completer = total_completer.set_index(\"LABEL_LVL_1_DESC\")\n",
    "    # print(total_completer) # total number of student completers for each section per year\n",
    "\n",
    "    completer = completer.drop(\"All Column Values\")\n",
    "    # print(completer) # total number of student completers for each section per county per year\n",
    "\n",
    "    # total rows before cleaning = 542,801\n",
    "    # total rows after cleaning = 8,353\n",
    "    \n",
    "    with pd.ExcelWriter('education_data.xlsx') as writer:\n",
    "        totals_attend.to_excel(writer, sheet_name='total_attendance')\n",
    "        attend.to_excel(writer, sheet_name='attendance')\n",
    "        dropout.to_excel(writer, sheet_name = 'dropout')\n",
    "        total_dropout.to_excel(writer, sheet_name = 'total_dropout')\n",
    "        total_hope.to_excel(writer, sheet_name = 'total_hope')\n",
    "        hope.to_excel(writer, sheet_name = 'hope')\n",
    "        total_completer.to_excel(writer, sheet_name = 'total_completer')\n",
    "        completer.to_excel(writer, sheet_name = 'completer')\n",
    "    \n",
    "    print(attend)\n",
    "    print(dropout)\n",
    "    print(hope)\n",
    "    print(completer)\n",
    "    \n",
    "############ Function Call ############\n",
    "data_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794L4vGXFdYw"
   },
   "source": [
    "## Web Collection Requirement \\#1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vXwpJObDFiWM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 1994   1995   1996   1997   1998   1999   2000   2001   2002  \\\n",
      "Counties                                                                        \n",
      "Appling            67     55     68     72     59     60     80     53     56   \n",
      "Atkinson           34     42     49     57     37     48     43     31     45   \n",
      "Bacon              48     50     49     42     37     34     44     32     25   \n",
      "Baker              11     14     10     12     12      8     11     13     10   \n",
      "Baldwin           164    151    141    131    137    146    129    112     95   \n",
      "...               ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "Wilcox             31     41     32     33     29     20     14     35     18   \n",
      "Wilkes             38     35     42     44     46     42     29     31     19   \n",
      "Wilkinson          25     29     28     41     37     40     31     40     25   \n",
      "Worth              77     86     68     74     77     80     74     62     55   \n",
      "County Summary  25181  25511  25381  25201  24496  24682  23927  23267  22000   \n",
      "\n",
      "                 2003  ...   2012   2013   2014   2015   2016   2017   2018  \\\n",
      "Counties               ...                                                    \n",
      "Appling            57  ...     43     38     32     36     35     45     26   \n",
      "Atkinson           24  ...     21     26     21     25     15     15     10   \n",
      "Bacon              20  ...     36     15     21     15     16     14     14   \n",
      "Baker               4  ...      4      4      2      4      3      5      2   \n",
      "Baldwin           126  ...     85     67     50     46     41     46     49   \n",
      "...               ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "Wilcox             24  ...     12     21     19      8     18     14     12   \n",
      "Wilkes             28  ...     16      8     15     11     11     14     12   \n",
      "Wilkinson          37  ...     19     11     22      7      6     10     10   \n",
      "Worth              61  ...     41     32     40     23     28     25     31   \n",
      "County Summary  21557  ...  15368  13818  12804  11748  11269  10467  10145   \n",
      "\n",
      "                2019  2020  Years Total  \n",
      "Counties                                 \n",
      "Appling           23    19         1357  \n",
      "Atkinson          15    10          796  \n",
      "Bacon             18    15          815  \n",
      "Baker              2     2          182  \n",
      "Baldwin           44    65         2720  \n",
      "...              ...   ...          ...  \n",
      "Wilcox             9     4          549  \n",
      "Wilkes            10     5          646  \n",
      "Wilkinson         10     8          606  \n",
      "Worth             28    26         1434  \n",
      "County Summary  9955  9250       515455  \n",
      "\n",
      "[160 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "# import sys\n",
    "# sys.path.insert(0,r'C:\\Users\\chett\\OneDrive\\Documents\\GEORGIA TECH\\2021 FALL\\CS2316\\FINAL PROJECT\\chromedriver.exe')\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "# from selenium.webdriver.firefox.service import Service\n",
    "\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def web_parser1():\n",
    "    \n",
    "    website = 'https://oasis.state.ga.us/oasis/webquery/qryPregnancy.aspx'\n",
    "    path = r\"C:\\Users\\chett\\OneDrive\\Documents\\GEORGIA TECH\\2021 FALL\\CS2316\\FINAL PROJECT\\chromedriver.exe\" # will need to change for each person\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage') \n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    \n",
    "    driver = webdriver.Chrome(path, options = options)  # Optional argument, if not specified will search path.\n",
    "    driver.get(website);\n",
    "    time.sleep(5) # Let the user actually see something!\n",
    "\n",
    "#     def get_prego_html():\n",
    "    preg_type = Select(driver.find_element(By.ID,'drpMeasure'))\n",
    "    preg_type.select_by_index(0)\n",
    "\n",
    "    ages = Select(driver.find_element(By.ID,'lstAge'))\n",
    "    ages.deselect_by_index(0)\n",
    "    for i in range(1, 4):\n",
    "        ages.select_by_index(i)\n",
    "\n",
    "    times = Select(driver.find_element(By.ID,'lstTime'))\n",
    "    for i in range(1, 27):\n",
    "        times.select_by_index(i)\n",
    "\n",
    "    counties = Select(driver.find_element(By.ID,'lstGeographies'))\n",
    "    counties.deselect_by_index(0)\n",
    "    for i in range(3,162):\n",
    "        counties.select_by_index(i)\n",
    "\n",
    "    races = Select(driver.find_element(By.ID,'lstRace'))\n",
    "    races.select_by_index(0)\n",
    "\n",
    "    ethnicities = Select(driver.find_element(By.ID,'lstEthnicity'))\n",
    "    ethnicities.select_by_index(0)\n",
    "\n",
    "    data_button = driver.find_element(By.ID,'imgSubmit')\n",
    "    data_button.click()\n",
    "    prego_html = driver.page_source\n",
    "#         return driver.page_source\n",
    "\n",
    "#     def scrape_prego(page_source):\n",
    "    soup = BeautifulSoup(prego_html, 'html')\n",
    "    \n",
    "    table = soup.find_all('table', {'id': 'Table2'})\n",
    "    \n",
    "    df = pd.DataFrame(pd.read_html(str(table))[0])\n",
    "    \n",
    "    prego_df = df\n",
    "#     return df\n",
    "\n",
    "#     def clean_prego(prego_df):\n",
    "    prego_df = prego_df.set_index(('Geography', 'Unnamed: 0_level_1'))\n",
    "    \n",
    "    labels = [i for i in range(1994, 2021)]\n",
    "    labels.append('Years Total')\n",
    "    \n",
    "    prego_df.columns = labels\n",
    "    prego_df.index.rename('Counties', inplace = True)\n",
    "    \n",
    "    cleaned_prego_data = prego_df\n",
    "#         return prego_df\n",
    "\n",
    "    # website = 'https://oasis.state.ga.us/oasis/webquery/qryPregnancy.aspx'\n",
    "    # service = Service('/Users/emilymcnichols/.wdm/drivers/geckodriver/macos/v0.30.0/geckodriver') # will need to change                                                                      # for each person...\n",
    "    # options = Options()\n",
    "    # options.headless = True\n",
    "    # options.private = True\n",
    "    # driver = webdriver.Firefox(options = options, service = service)\n",
    "    # driver.get(website)\n",
    "\n",
    "#     prego_html = get_prego_html()\n",
    "#     prego_data = scrape_prego(prego_html)\n",
    "#     cleaned_prego_data = clean_prego(prego_data)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    print(cleaned_prego_data)\n",
    "    cleaned_prego_data.to_excel('teen_pregnancy_data.xlsx')\n",
    "\n",
    "############ Function Call ############\n",
    "web_parser1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDD6sMsCXRxc"
   },
   "source": [
    "## Web Collection Requirement \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HAkUOqMgXQJG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 657/657 [43:18<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def web_parser2():\n",
    "    # will always remain the same\n",
    "    base_url = 'https://api.usa.gov/crime/fbi/sapi/'\n",
    "    api_key = 'lmDsvkVlIygKCujwFc6L2YG7f0ZzbuWEhyt2qafm'\n",
    "\n",
    "    # changes depending on what we want to access\n",
    "    agency_endpoint = '/api/agencies'\n",
    "\n",
    "    # ori: value of agency\n",
    "    ori = 'GA0360100'\n",
    "\n",
    "    # offense: offense type\n",
    "    # offense = 'aggravated-assault'\n",
    "    # offense = 'all-offenses'\n",
    "    offense = 'violent-crime'\n",
    "\n",
    "    # state_abbr\n",
    "    state_abbr = 'GA'\n",
    "\n",
    "    # variable\n",
    "    variable = 'count' # other avaliable: age, ethnicity, race, sex\n",
    "\n",
    "    # since: starting year\n",
    "    since = 2010\n",
    "\n",
    "    # until: ending year\n",
    "    until = 2020\n",
    "\n",
    "    # endpoint = f'/api/summarized/agencies/{ori}/{offense}/{since}/{until}'\n",
    "    # endpoint = f'/api/data/nibrs/{offense}/offender/states/{state_abbr}/{variable}'\n",
    "    # endpoint = f'/api/summarized/state/{state_abbr}/{offense}/{since}/{until}'\n",
    "    endpoint = f'/api/agencies/byStateAbbr/raw/{state_abbr}' # for agency (ORI) numbers\n",
    "    \n",
    "    # res = requests.get(f\"{base_url}{agency_endpoint}?api_key={api_key}\")\n",
    "    res = requests.get(f'{base_url}{endpoint}?api_key={api_key}')\n",
    "    agency_data = res.json()\n",
    "    \n",
    "    return agency_data\n",
    "\n",
    "def agencies(agency_data):\n",
    "    ori_list = list(agency_data['GA'].keys())\n",
    "    county_dict = {}\n",
    "\n",
    "    for ori in ori_list:\n",
    "        if agency_data['GA'][ori]['county_name'] in county_dict:\n",
    "            county_dict[agency_data['GA'][ori]['county_name']].append(ori)\n",
    "\n",
    "        elif agency_data['GA'][ori]['county_name'] == '' and 'NO COUNTY' not in county_dict:\n",
    "            county_dict['NO COUNTY'] = [ori]\n",
    "\n",
    "        elif agency_data['GA'][ori]['county_name'] == '':\n",
    "            county_dict['NO COUNTY'].append(ori)\n",
    "        else:\n",
    "            county_dict[agency_data['GA'][ori]['county_name']] = [ori]\n",
    "\n",
    "    return ori_list, county_dict\n",
    "\n",
    "def get_offenses(ori_list, agency_data):    \n",
    "    # If needed, we can try saving things to files as we go...maybe\n",
    "\n",
    "    # json.dump\n",
    "\n",
    "    # will always remain the same\n",
    "    base_url = 'https://api.usa.gov/crime/fbi/sapi/'\n",
    "    api_key = 'lmDsvkVlIygKCujwFc6L2YG7f0ZzbuWEhyt2qafm'\n",
    "    offense = 'violent-crime'\n",
    "    variable = 'count'\n",
    "\n",
    "    since = 2010\n",
    "    until = 2020\n",
    "\n",
    "    ori_dict = {}\n",
    "\n",
    "    # Add in a conditional that looks for 'Police' or 'Sheriff' in the key 'agency-name',\n",
    "    # if present, add to ori_list\n",
    "\n",
    "    # if \"Police D\" in agency_data['GA'][ori]['agency-name'] or \"Sheriff\"  in agency_data['GA'][ori]['agency-name']:\n",
    "\n",
    "    # slicing for testing but will need to change it to just ori_list\n",
    "    for ori in tqdm(ori_list):\n",
    "        years_dict = {i: 0 for i in range(2010, 2021)}\n",
    "\n",
    "        if \"Police D\" in agency_data['GA'][ori]['agency_name'] or \"Sheriff\" in agency_data['GA'][ori]['agency_name']:\n",
    "            endpoint = f'/api/summarized/agencies/{ori}/offenses/{since}/{until}'\n",
    "            # print('trying ori')\n",
    "            res = requests.get(f'{base_url}{endpoint}?api_key={api_key}')\n",
    "            # print('success', res.status_code)\n",
    "            ori_data = res.json()['results']\n",
    "\n",
    "            for i in ori_data:\n",
    "                years_dict[i['data_year']] += i['actual']\n",
    "\n",
    "            ori_dict[ori] = years_dict\n",
    "\n",
    "\n",
    "    with open('offenses-data.json', 'w') as fout:\n",
    "        json.dump(ori_dict, fout)\n",
    "\n",
    "\n",
    "        # Maybe load into a file so that we don't have to keep making the requests???\n",
    "\n",
    "        # json.dump(ori_dict, open(f'{}'))\n",
    "\n",
    "    return ori_dict\n",
    "\n",
    "def county_json(counties, json_data):\n",
    "    json_keys = tuple(json_data.keys())\n",
    "    county_keys = tuple(counties.keys())\n",
    "\n",
    "    county_offenses = {}\n",
    "    for i in range(len(county_keys)):\n",
    "        county_ori = []\n",
    "        for j in range(len(json_keys)):\n",
    "            if json_keys[j] in counties[county_keys[i]]:\n",
    "                county_ori.append(json_data[json_keys[j]])\n",
    "\n",
    "        county_offenses[county_keys[i]] = county_ori\n",
    "\n",
    "    return county_offenses\n",
    "\n",
    "def pd_county(county_data):\n",
    "    final = {}\n",
    "    for county in county_data:\n",
    "        if len(county_data[county]) == 0:\n",
    "            final[county] = [None for i in range(11)]\n",
    "        else:\n",
    "            a_list = []\n",
    "            for i in range(10, 21):\n",
    "                year_str = int(f'20{i}')\n",
    "                a_list.append(county_data[county][0][year_str])\n",
    "            final[county] = a_list\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final, orient = 'index', columns = [i for i in range(2010, 2021)])\n",
    "\n",
    "    return df\n",
    "\n",
    "############ Function Call ############\n",
    "# web_parser2()\n",
    "api_data = web_parser2()\n",
    "ori_list, county_dict = agencies(api_data)\n",
    "\n",
    "results = get_offenses(ori_list, api_data)\n",
    "county_data = county_json(county_dict, results)\n",
    "crime_df = pd_county(county_data)\n",
    "\n",
    "print(crime_df)\n",
    "crime_df.to_excel(\"crime_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezovwa1tp0we"
   },
   "source": [
    "## Additional Dataset Parsing/Cleaning Functions\n",
    "\n",
    "Write any supplemental (optional) functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4-s72RNuKLR"
   },
   "outputs": [],
   "source": [
    "def extra_source1():\n",
    "    pass\n",
    "\n",
    "    \n",
    "############ Function Call ############\n",
    "extra_source1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB3qXt_XuY7b"
   },
   "outputs": [],
   "source": [
    "# Define further extra source functions as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttEYrm9US5s"
   },
   "source": [
    "# Inconsistencies\n",
    "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
    "\n",
    "1. In the downloaded dataset, within the dropout data and hope data when we made the data into a pandas DataFrame there were a lot of NaN values within the columns. In this case we used the .dropna() method to get rid of the rows with NaN values. We dropped the rows because we wouldn't be taking the mean of the columns. Otherwise, it wouldv've been better to use the .fillna(0) method to make them 0 instead of dropping them.\n",
    "\n",
    "2. In the downloaded dataset, within the high school completer data there were strings and integer values in the \"PROGRAM_TOTAL\" column. We had to drop each row that had \"TFS\" in that column instead of a count of students that completed high school.\n",
    "\n",
    "3. In the downloaded dataset, within the attendance data, some of the files were inconsistent in their types. We had to convert the .xlsx files to be .csv files in order to use them all in one dataframe. Also, keeping the file types consistent lets us use all the years of data instead of leaving some out.\n",
    "\n",
    "4. In the Web Collection Requirement #1 dataset, the column names in the pandas DataFrame were tuples. To deal with this, we changed the column names using the .rename() method from tuples to strings to keep it consistent and easier to access.\n",
    "\n",
    "5. In the Web Collection Requirement #2 dataset, some data values within the gathered set of agencies were empty. To deal with this we filtered the data to not use that portion of the data since we didn't need it. Also, within this dataset there are only so many endpoints the FBI Crime Data API gives us to gather information. Therefore, we had to send many requests to the API to gather data about offenses for each agency. The API response rate is extremely slow and cannot handle many requests so this has been an ongoing issue. To deal with this we have had to slice the GA ori list (which is the identifier of an agency) and only do so many requests at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Collection Requirement #1\n",
    "\n",
    "Attached below is a photo of the pandas DataFrame for Web Collection Requirement #1. My partner worked on the code in a separate jupyter notebook using Firefox, and the code works. However, with setting up a driver, the path must be added and changed for each person. I added the path within my Environment Variables and it still would not excecute. I do not know how to add a path in Google Colab, and get the code to excecute on Google Colab.\n",
    "\n",
    "![alt text](cleaned_prego_data.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Collection Requirement #2\n",
    "\n",
    "Attached below is a photo of the pandas DataFrame for Web Collection Requirement #2. The FBI Crime Data API has an extremely slow response rate and crashes with the amount of requests we send. We have optimized our code and it logically makes sense; however, we are not able to get all of the data within the dataframe. Attached below is a picture of a sliced piece of the data we were able to get.\n",
    "\n",
    "![alt text](crime_data.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhaseII.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
